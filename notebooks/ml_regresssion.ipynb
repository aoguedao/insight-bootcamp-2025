{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ten baseline variables, age, sex, body mass index, average blood pressure, and six blood serum measurements were obtained for each of n = 442 diabetes patients, as well as the response of interest, a quantitative measure of disease progression one year after baseline.\n",
    "\n",
    "Source: https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>bp</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.038076</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.061696</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>-0.044223</td>\n",
       "      <td>-0.034821</td>\n",
       "      <td>-0.043401</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.019907</td>\n",
       "      <td>-0.017646</td>\n",
       "      <td>151.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.001882</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.051474</td>\n",
       "      <td>-0.026328</td>\n",
       "      <td>-0.008449</td>\n",
       "      <td>-0.019163</td>\n",
       "      <td>0.074412</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.068332</td>\n",
       "      <td>-0.092204</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.085299</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.044451</td>\n",
       "      <td>-0.005670</td>\n",
       "      <td>-0.045599</td>\n",
       "      <td>-0.034194</td>\n",
       "      <td>-0.032356</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.002861</td>\n",
       "      <td>-0.025930</td>\n",
       "      <td>141.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.089063</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.011595</td>\n",
       "      <td>-0.036656</td>\n",
       "      <td>0.012191</td>\n",
       "      <td>0.024991</td>\n",
       "      <td>-0.036038</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.022688</td>\n",
       "      <td>-0.009362</td>\n",
       "      <td>206.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005383</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.036385</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>0.003935</td>\n",
       "      <td>0.015596</td>\n",
       "      <td>0.008142</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.031988</td>\n",
       "      <td>-0.046641</td>\n",
       "      <td>135.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age       sex       bmi        bp        s1        s2        s3  \\\n",
       "0  0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401   \n",
       "1 -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412   \n",
       "2  0.085299  0.050680  0.044451 -0.005670 -0.045599 -0.034194 -0.032356   \n",
       "3 -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038   \n",
       "4  0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142   \n",
       "\n",
       "         s4        s5        s6  target  \n",
       "0 -0.002592  0.019907 -0.017646   151.0  \n",
       "1 -0.039493 -0.068332 -0.092204    75.0  \n",
       "2 -0.002592  0.002861 -0.025930   141.0  \n",
       "3  0.034309  0.022688 -0.009362   206.0  \n",
       "4 -0.002592 -0.031988 -0.046641   135.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes_X, diabetes_y = datasets.load_diabetes(return_X_y=True, as_frame=True)\n",
    "diabetes = pd.concat([diabetes_X, diabetes_y], axis=1)\n",
    "diabetes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First 10 columns are numeric predictive values\n",
    "\n",
    "Column 11 is a quantitative measure of disease progression one year after baseline\n",
    "\n",
    "\n",
    "| Feature       | Description     |\n",
    "| :------------- | :----------: |\n",
    "| age | age in years|\n",
    "| sex | sex |\n",
    "| bmi | body mass index|\n",
    "| bp | average blood pressure|\n",
    "| s1 | tc, T-Cells (a type of white blood cells)|\n",
    "| s2 | ldl, low-density lipoproteins|\n",
    "| s3 | hdl, high-density lipoproteins|\n",
    "| s4 | tch, thyroid stimulating hormone|\n",
    "| s5 | ltg, lamotrigine|\n",
    "| s6 | glu, blood sugar level|\n",
    "\n",
    "Note: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times the square root of n_samples (i.e. the sum of squares of each column totals 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first approach is to work with a linear model. Let's suppose we have $n$ samples of data and each sample $x^{(i)} \\in \\mathbb{R}^{p \\times 1}$  where $p$ denotes the number of features. Then\n",
    "\n",
    "$$x^{(i)} = (x^{(i)}_1, ..., x^{(i)}_n)^\\top \\qquad, \\forall i=1,\\dots, \\, n$$\n",
    "\n",
    "And for each $x^{(i)}$ we know the target value denoted by $y^{(i)} \\in \\mathbb{R}$, $i=1,\\dots, n$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's consider a lineal function $h$ and a vector of coefficients $\\beta \\in \\mathbb{R}^{(p+1) \\times 1}$ such that\n",
    "\n",
    "$$\\begin{align*}\n",
    "f_\\beta(x^{(i)}) &= \\beta_0 + \\beta_1 x^{(i)}_1 + \\beta_2 x^{(i)}_2 + ... + \\beta_p x^{(i)}_p \\\\\n",
    "    &= \\begin{bmatrix} 1 & x^{(i)}_1 & x^{(i)}_2 & \\dots & x^{(i)}_p\\end{bmatrix}\n",
    "        \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_p\\end{bmatrix}  \\\\\n",
    "    &= \\left(x^{(i)}\\right)^\\top \\beta \\qquad \\left(\\text{where} \\quad x^{(i)}_0 = 1\\right)\n",
    "\\end{align*}  $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's define the matrix of inputs $X \\in \\mathbb{R}^{n \\times (p + 1)} $, also know as design matrix, model matrix or regressor matrix, and the output vector $Y \\in \\mathbb{R}^n$ such that \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "X = \n",
    "\\begin{bmatrix} \n",
    "(x^{(1)})^\\top  \\\\ \n",
    "(x^{(2)})^\\top  \\\\\n",
    "\\vdots \\\\\n",
    "(x^{(n)})^\\top  \\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix} \n",
    "1 & x^{(1)}_1 & \\dots & x^{(1)}_p \\\\ \n",
    "1 & x^{(2)}_1 & \\dots & x^{(2)}_p \\\\\n",
    "\\vdots & \\vdots & & \\vdots \\\\\n",
    "1 & x^{(n)}_1 & \\dots & x^{(n)}_p \\\\\n",
    "\\end{bmatrix} \n",
    "\\qquad\n",
    "\\text{and}\n",
    "\\qquad\n",
    "Y = \\begin{bmatrix}y^{(1)} \\\\ y^{(2)} \\\\ \\vdots \\\\ y^{(n)}\\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Notice that now we can write this as a matrix operation\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "X \\beta &= \n",
    "\\begin{bmatrix}\n",
    "1 & x_1^{(1)} & ... & x_p^{(1)} \\\\\n",
    "\\vdots & \\vdots & & \\vdots \\\\\n",
    "1 & x_1^{(n)} & ... & x_p^{(n)} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{bmatrix} \n",
    " = \n",
    "\\begin{bmatrix}\n",
    "\\beta_0 + x^{(1)}_1 \\beta_1 + ... + x^{(1)}_p \\beta_p \\\\\n",
    "\\vdots \\\\\n",
    "\\beta_0 + x^{(n)}_1 \\beta_1 + ... + x^{(n)}_p \\beta_p \\\\\n",
    "\\end{bmatrix} \n",
    " = \n",
    "\\begin{bmatrix}\n",
    "f_\\beta(x^{(1)}) \\\\\n",
    "\\vdots \\\\\n",
    "f_\\beta(x^{(n)})\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Then our goal is to find an adequate vector of coefficients $\\beta$ such taht\n",
    "$$\n",
    "\\begin{bmatrix}y^{(1)} \\\\ y^{(2)} \\\\ \\vdots \\\\ y^{(n)}\\end{bmatrix}\n",
    "\\approx\n",
    "\\begin{bmatrix}\n",
    "f_\\beta(x^{(1)}) \\\\\n",
    "f_\\beta(x^{(2)}) \\\\\n",
    "\\vdots \\\\\n",
    "f_\\beta(x^{(n)})\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "that is equivalent to\n",
    "$$Y \\approx X \\beta $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{\\beta} = (X^\\top X)^{-1} X^\\top Y\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = diabetes_y.to_numpy().reshape(-1, 1)\n",
    "X = diabetes_X.to_numpy()\n",
    "# X = np.hstack((np.ones_like(y), X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -10.0098663 ],\n",
       "       [-239.81564367],\n",
       "       [ 519.84592005],\n",
       "       [ 324.3846455 ],\n",
       "       [-792.17563855],\n",
       "       [ 476.73902101],\n",
       "       [ 101.04326794],\n",
       "       [ 177.06323767],\n",
       "       [ 751.27369956],\n",
       "       [  67.62669218]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_naive = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "beta_naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -10.0098663 ],\n",
       "       [-239.81564367],\n",
       "       [ 519.84592005],\n",
       "       [ 324.3846455 ],\n",
       "       [-792.17563855],\n",
       "       [ 476.73902101],\n",
       "       [ 101.04326794],\n",
       "       [ 177.06323767],\n",
       "       [ 751.27369956],\n",
       "       [  67.62669218]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_ls, _, _, _ = np.linalg.lstsq(X, y)\n",
    "beta_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilistic Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following model\n",
    "\n",
    "$$ Y = X \\beta + \\varepsilon $$\n",
    "\n",
    "where $\\varepsilon \\sim  \\mathcal{N}(0, \\sigma^2 I)$.\n",
    "\n",
    "Note that $\\beta$ is not a random variable and then $Y \\ | \\ X \\sim \\mathcal{N}(X \\beta, \\sigma^2 I)$.\n",
    "\n",
    "\n",
    "The likelihood function for such random variable is\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\left( 2 \\pi \\sigma^2 \\right)^{-n/2} \\, \\exp\\left(- \\frac{1}{2 \\sigma ^2} || Y - X \\theta ||^2 \\right)\n",
    "$$\n",
    "\n",
    "Next, the log-likelihood (ignoring constants terms) is as follows\n",
    "$$\n",
    "l(\\beta) = -\\frac{n}{2} \\log \\sigma^2 - \\frac{1}{2 \\sigma ^2} || Y - X \\beta ||^2\n",
    "$$\n",
    "\n",
    "To estimate $\\beta$ we employ the Maximum Likelihood Estimation (MLE) method. Deriving $l(\\beta)$ with respect to $\\beta$ we obtain\n",
    "\n",
    "$$\n",
    "\\frac{\\partial l(\\beta)}{\\partial \\beta}= - \\frac{1}{\\sigma ^2} \\left( X^T Y + X^T X \\beta \\right)\n",
    "$$\n",
    "\n",
    "Equalling $\\partial l(\\beta) / \\partial \\beta = 0$ we show that\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{MLE} = (X^T X)^{-1} X^T Y\n",
    "$$.\n",
    "\n",
    "Finally, we need to show it is a maximum (homework)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our cost function\n",
    "\n",
    "$$J(\\beta) = \\frac{1}{2} \\left\\lVert Y - X \\beta \\right\\rVert^2_2 =  \\frac{1}{2} \\sum_{i=1}^{n} \\left( y^{(i)} - f_\\beta(x^{(i)})\\right)^2$$\n",
    "\n",
    "The goal is to solve the following optimization problem\n",
    "\n",
    "$$\\min_{\\beta} J(\\beta) = \\min_{\\beta} \\frac{1}{2}  \\left\\lVert Y - X \\beta \\right\\rVert^2_2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Gradient descent is an optimization algorithm which is commonly-used to train machine learning models. The idea is to take repeated steps in the opposite direction of the gradient because this is the direction of steepest descent. \n",
    "\n",
    "The algorithm can be written as\n",
    "\n",
    "$$\\beta^{(n+1)} = \\beta^{(n)} - \\alpha \\nabla_{\\beta} J(\\beta^{(n)})$$\n",
    "\n",
    "where $\\alpha >0$ is called learning rate. Now, let's compute the gradient of $J(\\beta)$. For any $k = 0, \\ldots, p$ we have\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial J(\\beta)}{\\partial \\beta_k} &=\n",
    "\\frac{\\partial }{\\partial \\beta_k} \\frac{1}{2} \\sum_{i=1}^{n} \\left(  y^{(i)} - f_{\\beta}(x^{(i)})\\right)^2 \\\\\n",
    "&= \\frac{1}{2} \\sum_{i=1}^{m}  2 \\left(  y^{(i)} - f_{\\beta}(x^{(i)}) \\right) \\frac{\\partial f_{\\beta}(x^{(i)})}{\\partial \\beta_k}\\\\\n",
    "&= \\sum_{i=1}^{n} \\left(  y^{(i)} - f_{\\beta}(x^{(i)})\\right) x^{(i)}_k\\end{aligned}$$\n",
    "\n",
    "Then\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\nabla_{\\beta} J(\\beta)\n",
    "    &= \\left(\n",
    "        \\sum_{i=1}^{n} \\left(  y^{(i)} - f_{\\beta}(x^{(i)})\\right) x^{(i)}_0,\n",
    "        \\dots,\n",
    "        \\sum_{i=1}^{n} \\left(  y^{(i)} - f_{\\beta}(x^{(i)})\\right) x^{(i)}_p\n",
    "    \\right)^\\top = X^\\top \\left( Y - X \\beta \\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Note that $\\alpha$ is a parameter from the algorithm and not from the model itself. These kind of parameters are usually named as __hyper-parameters__ and to find the best one is a big task itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  -8.51919013],\n",
       "       [-238.1434401 ],\n",
       "       [ 523.61476308],\n",
       "       [ 322.91326916],\n",
       "       [-467.92000419],\n",
       "       [ 219.43749726],\n",
       "       [ -43.96758555],\n",
       "       [ 135.69463421],\n",
       "       [ 630.44900597],\n",
       "       [  68.81894413]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_iter = 10_000\n",
    "lr = 1e-2\n",
    "tol = 1e-6\n",
    "\n",
    "n, m = X.shape  # Rows and columns\n",
    "beta = np.random.rand(m, 1)\n",
    "for i in range(1, max_iter + 1):\n",
    "  # Restart gradient\n",
    "  grad = np.zeros_like(beta, dtype=np.float64)\n",
    "  for x_i, y_i in zip(X, y.flatten()):\n",
    "    f_i = (x_i @ beta)  # Linear model evaluation\n",
    "    # print(f_i)\n",
    "    grad += (f_i - y_i) * x_i.reshape(-1, 1)\n",
    "  beta_new = beta - lr * grad  # Update parameters\n",
    "  # Stop criteria\n",
    "  rel_error = np.linalg.norm(beta - beta_new) / (np.linalg.norm(beta) + 1e-12)\n",
    "  if rel_error < tol:\n",
    "    print(f\"Converged! {rel_error:.2e} error in the {i}-th iteration\")\n",
    "    break\n",
    "  else:\n",
    "    beta = beta_new  # Update params for tolerance comparison\n",
    "\n",
    "print(i)\n",
    "beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients:\n",
      " [ -10.0098663  -239.81564367  519.84592005  324.3846455  -792.17563855\n",
      "  476.73902101  101.04326794  177.06323767  751.27369956   67.62669218]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression(fit_intercept=False)\n",
    "model.fit(diabetes_X, diabetes_y)\n",
    "print(f\"Coefficients:\\n {model.coef_.T}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge regression shrinks the regression coefficients by imposing a penalty on their size. The ridge coefficients minimize a penalized residual sum of squares,\n",
    "$$\n",
    "\\min_\\beta J(\\beta) = \\min_{\\beta} \\frac{1}{2}  \\left\\lVert y - X \\beta \\right\\rVert^2_2 + \\frac{\\alpha}{2} \\left\\lVert \\beta \\right\\rVert_2^2\n",
    "$$\n",
    "where $\\alpha > 0$ is a complexity parameter that controls the amount of shrinkage. This penalization helps to deal with colinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients:\n",
      " [  20.13800709 -131.24149467  383.48370376  244.83506964  -15.18674139\n",
      "  -58.34413649 -174.84237091  121.9849503   328.4987567   110.8864333 ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge_model = Ridge(alpha=0.5)\n",
    "ridge_model.fit(diabetes_X, diabetes_y)\n",
    "print(f\"Coefficients:\\n {ridge_model.coef_.T}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso regression is also a regularization technique, but instead of penalizing using an $\\ell_2$-norm, it considers an $\\ell_1$-penalization. It is useful in some contexts due to its tendency to prefer solutions with fewer non-zero coefficients and even it could be utilized as a way to select features in a model.\n",
    "\n",
    "$$\n",
    "\\min_\\beta J(\\beta) = \\min_{\\beta} \\frac{1}{2n}  \\left\\lVert y - X \\beta \\right\\rVert^2_2 + \\alpha \\left\\lVert \\beta \\right\\rVert_1^2\n",
    "$$\n",
    "\n",
    "where $\\left\\lVert \\beta \\right\\rVert_1 = \\sum_{i=0}^p \\left\\lvert \\beta_i \\right\\rvert$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients:\n",
      " [  0.          -0.         471.04187427 136.50408382  -0.\n",
      "  -0.         -58.31901693   0.         408.0226847    0.        ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso_model = Lasso(alpha=0.5)\n",
    "lasso_model.fit(diabetes_X, diabetes_y)\n",
    "print(f\"Coefficients:\\n {lasso_model.coef_.T}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic-Net "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elastic-Net is a compromise between Ridge and Lasso regression, considering both $\\ell_1$ and $\\ell_2$-norm regularizaion. Let's consider a ratio $\\rho \\in (0,1)$, then\n",
    "\n",
    "$$\n",
    "\\min_\\beta J(\\beta) = \\min_{\\beta} \\frac{1}{2n}  \\left\\lVert y - X \\beta \\right\\rVert^2_2 + \\alpha \\rho \\left\\lVert \\beta \\right\\rVert_1^2 + \\frac{\\alpha (1 - \\rho)}{2} \\left\\lVert \\beta \\right\\rVert_2^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients:\n",
      " [ 1.55873597  0.          6.36640961  4.61688157  1.82562593  1.36291798\n",
      " -4.0457662   4.44665465  6.10117638  3.8868802 ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "elasticnet_model = ElasticNet(alpha=0.5, l1_ratio=0.4)\n",
    "elasticnet_model.fit(diabetes_X, diabetes_y)\n",
    "print(f\"Coefficients:\\n {elasticnet_model.coef_.T}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "math-bio-book",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
